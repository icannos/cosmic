{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d062f2d01452d75"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_ablation = pd.read_csv('../../output/corr_ablation_decoding.csv')\n",
    "df = pd.read_csv('../../output/corr_mi_tasks_wsh_rouge_2.csv')\n",
    "\n",
    "df = pd.concat([df, df_ablation])\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83deffaf9c0b349"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df[~df['metadata/Decoding config'].isna()]\n",
    "\n",
    "# df = df[~df['metadata/Decoding config'].str.contains('short')]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "# tasks names\n",
    "tasks_names = [c.split('/')[0] for c in df.columns if \"kl\" in c]\n",
    "print(tasks_names)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f3e7b809f0b7062f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "df['metadata/Dataset name'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5ee066b902d670d0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df['source'] = df[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fe51c58e2331570b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print decoding config"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5126c028f531b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "# Commmon metrics\n",
    "        \n",
    "        \n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5079584aab838ca1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def plot_correlation_matrix(df):\n",
    "    ROUGES = [\"common/rouge2\"]\n",
    "    MI = ['I(summary -> text)', 'I(text -> summary)']\n",
    "    SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "    \n",
    "\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\",\n",
    "                 \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "                 \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "                 }\n",
    "\n",
    "    classification_tasks = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "    \n",
    "    \n",
    "\n",
    "    # heatmap of correlation between MI and ROUGE\n",
    "\n",
    "    df_corr = df[~df['metadata/Decoding config'].str.contains(\"short\")][ROUGES + SHM + MI + classification_tasks].corr()\n",
    "\n",
    "    # make diverging colormap\n",
    "    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sns.heatmap(df_corr, annot=True, cmap=cmap, vmin=-1, vmax=1, center=0, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c80fdb528629a7a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make a table with the correlation between MI and ROUGE and the SHmetrics, grouped by dataset\n",
    "\n",
    "\n",
    "def make_correlation_table(df):\n",
    "    df =df.copy()\n",
    "\n",
    "    ROUGES = [\"common/rougeLsum\"]\n",
    "    MI = ['I(summary -> text)', 'I(text -> summary)']\n",
    "    SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\",\n",
    "                 \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "                 \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "                 }\n",
    "    \n",
    "    classification_tasks = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "    \n",
    "    # make proba_of_error proba_of_success\n",
    "    df[classification_tasks] = 1 - df[classification_tasks]\n",
    "    # rename\n",
    "    df = df.rename(columns={c + \"/proba_of_error\" : c + \"/proba_of_success\" for c in map_tasks.keys()})\n",
    "    \n",
    "    classification_tasks = [c + \"/proba_of_success\" for c in map_tasks.keys()]\n",
    "    \n",
    "    \n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")]\n",
    "    \n",
    "    print(df['metadata/Decoding config'].unique())\n",
    "    \n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "    \n",
    "    display(df)\n",
    "    datasets = set(df['metadata/Dataset name'].dropna().unique())\n",
    "    datasets -= set(['peer_read', 'arxiv'])\n",
    "    \n",
    "    print(datasets)\n",
    "\n",
    "    # create a dataframe with the correlation between MI and ROUGE and the SHmetrics, grouped by dataset\n",
    "    df_corr = pd.DataFrame(columns=['Dataset name', 'Metric', 'Correlation'])\n",
    "\n",
    "    for dataset in datasets:\n",
    "        # select dataset\n",
    "        df_dataset = df[df['metadata/Dataset name'] == dataset]\n",
    "        df_dataset = df_dataset[ROUGES + SHM + MI + classification_tasks].corr()\n",
    "        # add dataset name\n",
    "        df_dataset['Dataset name'] = dataset\n",
    "        \n",
    "        # add metric name\n",
    "        df_dataset['Metric'] = df_dataset.index\n",
    "        \n",
    "        # melt dataframe\n",
    "        df_dataset = df_dataset.melt(id_vars=['Dataset name', 'Metric'], var_name=\"Correlation\", value_name=\"Value\")\n",
    "        \n",
    "        # append to main dataframe\n",
    "        df_corr = df_corr.append(df_dataset)\n",
    "        \n",
    "\n",
    "    def rename_metrics(x):\n",
    "        splits = x.split('/')\n",
    "        \n",
    "        if len(splits) == 1:\n",
    "            if splits[0] == \"I(summary -> text)\":\n",
    "                return \"$I(T,S)$\"\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            if splits[0] in  map_tasks.keys():\n",
    "                return map_tasks[splits[0]]\n",
    "            else:\n",
    "                if splits[1] == \"rougeLsum\":\n",
    "                    return \"\\\\texttt{ROUGE-L}\"\n",
    "                else:\n",
    "                    return splits[1]\n",
    "        \n",
    "    df_corr = df_corr.pivot(index=['Dataset name', 'Metric'], columns='Correlation', values='Value')\n",
    "\n",
    "    # Keep shmetric only in columns\n",
    "    df_corr = pd.concat({'SH.' : df_corr[[c for c in df_corr.columns if \"SHMetric\" in c]], 'CT.' : df_corr[classification_tasks]}, axis=1)\n",
    "    \n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    # Select index to be displayed\n",
    "    df_corr = df_corr.loc[idx[:, ['common/rougeLsum', 'I(summary -> text)'] + SHM], :]\n",
    "    df_corr = df_corr.dropna()\n",
    "\n",
    "    # rename columns\n",
    "    df_corr.columns = pd.MultiIndex.from_tuples([(c[0].replace('_', '-'), rename_metrics(c[1])) for c in df_corr.columns])\n",
    "\n",
    "    df_corr = df_corr.reset_index()\n",
    "    # rename Metric\n",
    "    df_corr[('Metric', '')] = df_corr[('Metric', '')].apply(rename_metrics)\n",
    "\n",
    "    df_corr = df_corr.set_index([\"Dataset name\", 'Metric'])\n",
    "    df_corr = df_corr.sort_index()\n",
    "    \n",
    "    # Remove \"_\" from column names\n",
    "\n",
    "\n",
    "    return df_corr\n",
    "    \n",
    "table = make_correlation_table(df).transpose()\n",
    "\n",
    "\n",
    "table.columns = pd.MultiIndex.from_tuples([(c[0].replace('_', '-'), c[1]) for c in table.columns])\n",
    "\n",
    "style = table.style\n",
    "\n",
    "\n",
    "style = style.format(precision=2)\n",
    "style = style.format_index(escape=\"latex\", axis=0)\n",
    "\n",
    "# highlight max for each dataset with bfseries\n",
    "list_datasets = set(table.columns.get_level_values(0))\n",
    "list_metrics = set(table.columns.get_level_values(1))\n",
    "idx = pd.IndexSlice\n",
    "for dataset in list_datasets:\n",
    "    style = style.highlight_max(axis=1, subset=(idx[:], idx[dataset, :]), props='bfseries:')\n",
    "    \n",
    "# add background gradient\n",
    "style = style.background_gradient(cmap='viridis', vmin=0.2, vmax=1)\n",
    "# convert to latex\n",
    "path = \"../../../papers/Mutual-information-for-summarization/tables/correlation_table.tex\"\n",
    "# create parent\n",
    "Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "display(style)\n",
    "\n",
    "latex_code = style.to_latex(clines=\"skip-last;data\", sparse_index=True, sparse_columns=True, caption=\"Correlation between MI and ROUGE, and Seahorse metrics and probability of success of the classifcation task, grouped by datasets for non-trivial decoding strategies. SH. stands for Seahorse metrics and CT. for classification tasks.\", label=\"tab:correlation_table\", environment=\"table*\", hrules=True, convert_css=True)\n",
    "\n",
    "import re\n",
    "\n",
    "# add a resize box around the tabular\n",
    "latex_code = re.sub(r\"\\\\begin{tabular}\", r\"\\\\resizebox{0.7\\\\textwidth}{!}{\\\\begin{tabular}\", latex_code)\n",
    "latex_code = re.sub(r\"\\\\end{tabular}\", r\"\\\\end{tabular}}\", latex_code)\n",
    "\n",
    "# add centering to the table environment\n",
    "latex_code = re.sub(r\"\\\\begin{table\\*}\", r\"\\\\begin{table*}[h!]\\\\centering\", latex_code)\n",
    "\n",
    "\n",
    "# save latex code\n",
    "#with open(path, 'w') as f:\n",
    "#     f.write(latex_code)\n",
    "\n",
    "print(latex_code)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "94eb8d5599a7d91f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "\n",
    "ROUGES = [\"common/rougeLsum\"]\n",
    "MI = ['I(summary -> text)', 'I(text -> summary)']\n",
    "SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "\n",
    "\n",
    "map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\",\n",
    "             \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "             \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "             }\n",
    "\n",
    "classification_tasks_error = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "classification_tasks = [c + \"/proba_of_success\" for c in map_tasks.keys()]\n",
    "\n",
    "# make proba_of_error proba_of_success\n",
    "df[classification_tasks] = 1 - df[classification_tasks_error]\n",
    "\n",
    "def plot_multiple_datasets_correlations(df, COLS, name):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    df =df.copy()\n",
    "\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")]\n",
    "\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "\n",
    "    datasets = set(df['metadata/Dataset name'].dropna().unique())\n",
    "    datasets -= set(['peer_read', 'arxiv'])\n",
    "     \n",
    "     \n",
    "     \n",
    "    fig, axes = plt.subplots(len(datasets), len(COLS), figsize=(25, 10), sharey=False, sharex=False, dpi=300)\n",
    "     \n",
    "    def rename_cols(x):\n",
    "        if \"SHMetric\" in x:\n",
    "            return x.split('/')[1]\n",
    "        else:\n",
    "            return map_tasks[x.split('/')[0]]\n",
    "     \n",
    "    for idx, col in enumerate(COLS):\n",
    "        for didx, ds in enumerate(datasets):\n",
    "            group = df[df['metadata/Dataset name'] == ds]\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "            \n",
    "            \n",
    "            sns.regplot(data=group, x=\"I(summary -> text)\", y=col, ax=axes[didx,idx], x_ci=None, ci=False, scatter=False, line_kws={'alpha': 0.9, 'linewidth': 5})\n",
    "            sns.scatterplot(data=group, x=\"I(summary -> text)\", y=col, hue='metadata/Model name', ax=axes[didx,idx], palette='tab10', s=400, markers='o')\n",
    "            \n",
    "            \n",
    "            axes[didx,idx].set_xlabel(\"\")\n",
    "            if didx == 0:\n",
    "                axes[didx,idx].set_title(rename_cols(col), fontsize=22, fontweight='bold')\n",
    "\n",
    "            axes[didx,idx].set_ylabel(\"\")\n",
    "            if idx == 0:\n",
    "                axes[didx,idx].set_ylabel(ds, fontsize=22, fontweight='bold')\n",
    "             \n",
    "            # make xtick labels bigger\n",
    "            axes[didx,idx].tick_params(axis='x', labelsize=18)\n",
    "            axes[didx,idx].tick_params(axis='y', labelsize=18)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            # add grid\n",
    "            axes[didx,idx].grid(True, which='both', axis='both', linestyle='--')\n",
    "             \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # global legend below the figure\n",
    "    handles, labels = axes[0,0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.1), ncol=3, fontsize=18)\n",
    "    # remove all legends\n",
    "    for ax in axes.flatten():\n",
    "        ax.get_legend().remove()\n",
    "     \n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/multiple_datasets_correlations_{name}.png\"\n",
    "        # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "     \n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "     \n",
    "     \n",
    "\n",
    "plot_multiple_datasets_correlations(df, COLS=SHM, name=\"shmetrics\")\n",
    "plot_multiple_datasets_correlations(df, COLS=classification_tasks, name=\"classification_tasks\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1963816d07193afe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_classification_tasks_proba_kl(df, dataset):\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")].copy()\n",
    "    df = df[df['metadata/Dataset name'] == dataset]\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\",\n",
    "                 \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "                 \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "                 }\n",
    "    # select only the tasks we want\n",
    "\n",
    "    # create a discrete sequential color palette with viridis\n",
    "\n",
    "\n",
    "    def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "        sns.regplot(data=data, x=x, y=y, ci=None, scatter=False, ax=ax, x_ci='sd')\n",
    "        sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette=\"tab10\")\n",
    "        return ax\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(len(map_tasks), 2, figsize=(10, 7), sharey=False, sharex=False, dpi=300)\n",
    "\n",
    "    for tidx, task in enumerate(map_tasks.keys()):\n",
    "        topplot = df\n",
    "        # rename columns\n",
    "        topplot = topplot.rename(columns={\"metadata/Decoding size\": \"Decoding size\", \"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "        custom_reg_plot(data=topplot, x=\"I(summary -> text)\", y=f\"{task}/proba_of_error\", hue=\"Model name\", style='Model name', ax=axes[tidx, 0])\n",
    "        custom_reg_plot(data=topplot, x=\"I(summary -> text)\", y=f\"{task}/kl\", hue=\"Model name\", style='Model name', ax=axes[tidx, 1])\n",
    "\n",
    "        # annotate with r value\n",
    "        axes[tidx, 0].annotate(f\"r={topplot['I(summary -> text)'].corr(df[f'{task}/proba_of_error']):.2f}\", xy=(0.05, 0.2), xycoords='axes fraction', fontsize=12,\n",
    "                               horizontalalignment='left', verticalalignment='top')\n",
    "        axes[tidx, 1].annotate(f\"r={topplot['I(summary -> text)'].corr(df[f'{task}/kl']):.2f}\", xy=(0.05, 0.1), xycoords='axes fraction', fontsize=12,)\n",
    "\n",
    "\n",
    "\n",
    "        # add title\n",
    "        axes[tidx, 0].set_title(map_tasks[task], fontsize=20,  fontweight='bold')\n",
    "        axes[tidx, 1].set_title(map_tasks[task], fontsize=20, fontweight='bold')\n",
    "\n",
    "        # add y label\n",
    "        axes[tidx, 0].set_ylabel(\"P(error)\", fontsize=16, fontweight='bold')\n",
    "        axes[tidx, 1].set_ylabel(\"KL\", fontsize=16, fontweight='bold')\n",
    "\n",
    "\n",
    "\n",
    "    # add global legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.15), ncol=2, fontsize=16)\n",
    "    # remove all legends\n",
    "    for ax in axes.flatten():\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/classification_tasks/{dataset}_classification_tasks.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_classification_tasks_proba_kl(df, dataset=\"cnn_dailymail\")\n",
    "plot_classification_tasks_proba_kl(df, dataset=\"rotten_tomatoes\")\n",
    "plot_classification_tasks_proba_kl(df, dataset=\"xsum\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a0e1f980a7a55cd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_model_average_mi_per_dataset(df):\n",
    "    \n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")].copy()\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "\n",
    "\n",
    "    datasets = set(df['metadata/Dataset name'].dropna().unique())\n",
    "    datasets -= set(['peer_read', 'arxiv'])\n",
    "    \n",
    "    df = df[df['metadata/Dataset name'].isin(datasets)]\n",
    "    \n",
    "    # sort by I(summary -> text)\n",
    "    df = df.sort_values(by=\"I(summary -> text)\", ascending=False)\n",
    "    \n",
    "    # bar plot\n",
    "    \n",
    "    ax = sns.barplot(data=df, y=\"metadata/Model name\", x=\"I(summary -> text)\", hue=\"metadata/Dataset name\", orient=\"h\")\n",
    "    \n",
    "    # ylim\n",
    "    ax.set_xlim([40, 100])\n",
    "    \n",
    "    \n",
    "    \n",
    "plot_model_average_mi_per_dataset(df)\n",
    "    \n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "fde0f231963213c2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_classification_correlations(df, COL, path =\"test.png\"):\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].str.contains('short')]\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\",\n",
    "                 \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "                 \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "                 }\n",
    "    # select only the tasks we want\n",
    "\n",
    "    # create a discrete sequential color palette with viridis\n",
    "    # palette = sns.color_palette(\"viridis\", len(df['metadata/Decoding size'].unique()))\n",
    "\n",
    "\n",
    "\n",
    "    def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "        sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "        sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette=\"tab10\")\n",
    "        return ax\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(len(map_tasks), 2, figsize=(10, 7), sharey=False, sharex=True, dpi=300)\n",
    "\n",
    "    for tidx, task in enumerate(map_tasks.keys()):\n",
    "        topplot = df\n",
    "        # rename columns\n",
    "        topplot = topplot.rename(columns={\"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "        custom_reg_plot(data=topplot, x=COL, y=f\"{task}/proba_of_error\", hue=\"Model name\", style='Model name', ax=axes[tidx, 0])\n",
    "        custom_reg_plot(data=topplot, x=COL, y=f\"{task}/kl\", hue=\"Model name\", style='Model name', ax=axes[tidx, 1])\n",
    "\n",
    "        # annotate with r value\n",
    "        axes[tidx, 0].annotate(f\"r={topplot[COL].corr(df[f'{task}/proba_of_error']):.2f}\", xy=(0.05, 0.2), xycoords='axes fraction', fontsize=12,\n",
    "                               horizontalalignment='left', verticalalignment='top')\n",
    "        axes[tidx, 1].annotate(f\"r={topplot[COL].corr(df[f'{task}/kl']):.2f}\", xy=(0.05, 0.1), xycoords='axes fraction', fontsize=12, )\n",
    "\n",
    "\n",
    "\n",
    "        # add title\n",
    "        axes[tidx, 0].set_title(map_tasks[task], fontsize=20)\n",
    "        axes[tidx, 1].set_title(map_tasks[task], fontsize=20)\n",
    "\n",
    "        # add y label\n",
    "        axes[tidx, 0].set_ylabel(\"P(error)\", fontsize=16)\n",
    "        axes[tidx, 1].set_ylabel(\"KL\", fontsize=16)\n",
    "\n",
    "\n",
    "\n",
    "    # add global legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.3), ncol=2, fontsize=14)\n",
    "    # remove all legends\n",
    "    for ax in axes.flatten():\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # save figure\n",
    "    path = Path(f\"../../../papers/Mutual-information-for-summarization/img/\") / path\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_classification_correlations(df[df['metadata/Dataset name'] == \"peer_read\"], COL=\"common/rougeLsum\", path=\"rotten_tomaties.png\")\n",
    "plot_classification_correlations(df[df['metadata/Dataset name'] == \"peer_read\"], COL=\"I(summary -> text)\", path=\"rotten_tomaties.png\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29b22959d443c05b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def plot_seahorse_metrics(df, COL):\n",
    "    df = df[~df['metadata/Decoding config'].str.contains('short')]\n",
    "    def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "        sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "        sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette='mako')\n",
    "        return ax\n",
    "\n",
    "    tasks_sh = set([c.split('/')[1] for c in df.columns if \"SH\" in c])\n",
    "    metrics = set([c.split('/')[-1] for c in df.columns if \"SH\" in c])\n",
    "\n",
    "\n",
    "    fig, axes = plt.subplots(2, len(tasks_sh) // 2, figsize=(15, 7), sharey=True, sharex=False, dpi=300)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    toplot = df.rename(columns={ \"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "    for tidx, task in enumerate(tasks_sh):\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "        custom_reg_plot(data=toplot, x=COL, y=f\"SHMetric/{task}/proba_1\", hue=\"Model name\", style='Model name', ax=axes[tidx])\n",
    "        # annotate with r value\n",
    "        axes[tidx].annotate(f\"r={toplot[COL].corr(df[f'SHMetric/{task}/proba_1']):.2f}\", xy=(0.05, 0.95), xycoords='axes fraction', fontsize=16,\n",
    "                            horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "        # change y title to be more readable\n",
    "        axes[tidx].set_ylabel(\"P(yes)\", fontsize=16)\n",
    "        axes[tidx].set_xlabel(COL, fontsize=16)\n",
    "\n",
    "        # add title\n",
    "        axes[tidx].set_title(f\"{task}\", fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # add global legend\n",
    "    handles, labels = axes[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=3, fontsize=16)\n",
    "    # remove all legends\n",
    "    for ax in axes:\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    fig.tight_layout()\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/ablation_decoding_size/cnn_dailymail_sh_metrics.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "     # plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "plot_seahorse_metrics(df[df['metadata/Dataset name'] == \"rotten_tomatoes\"], COL=\"common/rougeLsum\")\n",
    "plot_seahorse_metrics(df[df['metadata/Dataset name'] == \"rotten_tomatoes\"], COL=\"I(summary -> text)\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2dddd1dc3334926b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26e18191cef5d806"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Other stuff\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f472b0db050e4c9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "# List of datasets\n",
    "datasets = df['metadata/Dataset name'].dropna().unique()\n",
    "\n",
    "for dataset in datasets:\n",
    "    \n",
    "    # plot I(summary -> text) for each model\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    # keep only top_p_sampling\n",
    "    # df_top_p = df[df['metadata/Decoding config'] == 'top_p_sampling']\n",
    "    # select dataset\n",
    "    df_top_p = df[df['metadata/Dataset name'] == dataset]\n",
    "    df_top_p = df[df['metadata/Decoding size'] == 50]\n",
    "    \n",
    "    # sort by I(summary -> text)\n",
    "    df_top_p = df_top_p.sort_values(by=\"I(summary -> text)\", ascending=False)\n",
    "    \n",
    "    ax =sns.barplot(data=df_top_p, y=\"metadata/Model name\", x=\"I(summary -> text)\", orient=\"h\")\n",
    "    \n",
    "    # change y title to be more readable\n",
    "    ax.set(ylabel=\"Model name\")\n",
    "    \n",
    "    # change xlim based to be a little bit less than the min and a little bit more than the max\n",
    "    ax.set_xlim([df_top_p['I(summary -> text)'].min() - 5, df_top_p['I(summary -> text)'].max() + 5])\n",
    "    \n",
    "    # save figure\n",
    "    #plt.savefig(f\"../../../papers/Mutual-information-for-summarization/img/model_comparison/{dataset}_top_p.png\", dpi=300, bbox_inches='tight')\n",
    "    # plt.clf()\n",
    "    \n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2c98e219f7c42d17"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "ddf = df.drop('Unnamed: 0', axis=1)\n",
    "# df = df.set_index([metadata for metadata in df.columns if \"metadata\" in metadata])\n",
    "ddf = ddf.melt(id_vars=[metadata for metadata in ddf.columns if \"metadata\" in metadata] ,var_name=\"Score\", value_name=\"Value\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "417f75e72fcd543d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "# palette = sns.color_palette(\"viridis\", len(df['metadata/Decoding size'].unique()))\n",
    "\n",
    "\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette=\"tab10\")\n",
    "    return ax\n",
    "\n",
    "\n",
    "df[\"I(summary -> text) - I(text -> summary)\"] =  df[\"I(summary -> text)\"] - df[\"I(text -> summary)\"]\n",
    "\n",
    "# COL = 'I(summary -> text) - I(text -> summary)'\n",
    "COL = \"I(summary -> text)\"\n",
    "\n",
    "tasks_sh = set([c.split('/')[1] for c in df.columns if \"SH\" in c])\n",
    "metrics = set([c.split('/')[-1] for c in df.columns if \"SH\" in c])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "datasets = df['metadata/Dataset name'].dropna().unique()\n",
    "print(datasets)\n",
    "for ds in datasets:\n",
    "    toplot = df.rename(columns={\"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\", \"metadata/Dataset name\": \"Dataset name\"})\n",
    "    toplot = toplot[toplot['Dataset name'] == ds]\n",
    "    \n",
    "    display(toplot)\n",
    "\n",
    "    # fig, axes = plt.subplots(2, len(tasks_sh) // 2, figsize=(15, 7), sharey=True, sharex=False, dpi=300)\n",
    "    # axes = axes.flatten()\n",
    "    # for tidx, task in enumerate(tasks_sh):\n",
    "    #     sns.set_theme(style=\"whitegrid\")\n",
    "    # \n",
    "    #     custom_reg_plot(data=toplot, x=COL, y=f\"SHMetric/{task}/proba_1\", hue=\"Model name\", style='Model name', ax=axes[tidx])\n",
    "    #     # annotate with r value\n",
    "    #     axes[tidx].annotate(f\"r={toplot[COL].corr(df[f'SHMetric/{task}/proba_1']):.2f}\", xy=(0.05, 0.95), xycoords='axes fraction', fontsize=16,\n",
    "    #                         horizontalalignment='left', verticalalignment='top')\n",
    "    # \n",
    "    #     # change y title to be more readable\n",
    "    #     axes[tidx].set_ylabel(\"P(yes)\", fontsize=16)\n",
    "    #     axes[tidx].set_xlabel(COL, fontsize=16)\n",
    "    # \n",
    "    #     # add title\n",
    "    #     axes[tidx].set_title(f\"{task}\", fontsize=20)\n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # \n",
    "    # # add global legend\n",
    "    # handles, labels = axes[0].get_legend_handles_labels()\n",
    "    # fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=3, fontsize=16)\n",
    "    # # remove all legends\n",
    "    # for ax in axes:\n",
    "    #     if ax.get_legend():\n",
    "    #         ax.get_legend().remove()\n",
    "    # \n",
    "    # \n",
    "    # fig.tight_layout()\n",
    "    # # save figure\n",
    "    # path = f\"../../../papers/Mutual-information-for-summarization/img/tasks_perfs/{ds}_sh_metrics.png\"\n",
    "    # # create parent\n",
    "    # Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    # \n",
    "    # \n",
    "    # plt.savefig(path, dpi=300, bbox_inches='tight')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a2792bf28a97836"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ablation decoding size\n",
    "\n",
    "## SH metrics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c0180f9dda1f1d8f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette='mako')\n",
    "    return ax\n",
    "\n",
    "df = pd.read_csv('../../output/corr_ablation_shmetrics.csv')\n",
    "\n",
    "\n",
    "# keep only long and top p\n",
    "\n",
    "df['metadata/Decoding size'] = df['metadata/Decoding config'].apply(lambda x: x.split('_')[-1])\n",
    "# change type to int\n",
    "df['metadata/Decoding size'] = df['metadata/Decoding size'].astype(int)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61fb9f84da90c40f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "228d430a7d93a2df"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette='mako')\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "df[\"I(summary -> text) - I(text -> summary)\"] =  df[\"I(summary -> text)\"] - df[\"I(text -> summary)\"]\n",
    "\n",
    "# COL = 'I(summary -> text) - I(text -> summary)'\n",
    "COL = \"I(summary -> text)\"\n",
    "\n",
    "tasks_sh = set([c.split('/')[1] for c in df.columns if \"SH\" in c])\n",
    "metrics = set([c.split('/')[-1] for c in df.columns if \"SH\" in c])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, len(tasks_sh) // 2, figsize=(15, 7), sharey=True, sharex=False, dpi=300)\n",
    "axes = axes.flatten()\n",
    "\n",
    "toplot = df.rename(columns={\"metadata/Decoding size\": \"Decoding size\", \"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "for tidx, task in enumerate(tasks_sh):\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        \n",
    "        custom_reg_plot(data=toplot, x=COL, y=f\"SHMetric/{task}/proba_1\", hue=\"Decoding size\", style='Model name', ax=axes[tidx])\n",
    "        # annotate with r value\n",
    "        axes[tidx].annotate(f\"r={toplot[COL].corr(df[f'SHMetric/{task}/proba_1']):.2f}\", xy=(0.05, 0.95), xycoords='axes fraction', fontsize=16,\n",
    "                horizontalalignment='left', verticalalignment='top')\n",
    "        \n",
    "        # change y title to be more readable\n",
    "        axes[tidx].set_ylabel(\"P(yes)\", fontsize=16)\n",
    "        axes[tidx].set_xlabel(COL, fontsize=16)\n",
    "        \n",
    "        # add title\n",
    "        axes[tidx].set_title(f\"{task}\", fontsize=20)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "# add global legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=3, fontsize=16)\n",
    "# remove all legends\n",
    "for ax in axes:\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "fig.tight_layout()\n",
    "# save figure\n",
    "path = f\"../../../papers/Mutual-information-for-summarization/img/ablation_decoding_size/cnn_dailymail_sh_metrics.png\"\n",
    "# create parent\n",
    "Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "# plt.clf()\n",
    "#plt.close('all')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "58d33384e5368a75"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Classifications performance"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "609aab7ad8b5ce9f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = df[~df['metadata/Decoding config'].isna()]\n",
    "\n",
    "# df = df[~df['metadata/Decoding config'].str.contains('short')]\n",
    "display(df)\n",
    "print(df.columns)\n",
    "\n",
    "\n",
    "# tasks names\n",
    "tasks_names = [c.split('/')[0] for c in df.columns if \"kl\" in c]\n",
    "print(tasks_names)\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette='mako')\n",
    "    return ax\n",
    "\n",
    "\n",
    "df[\"I(summary -> text) - I(text -> summary)\"] =  df[\"I(summary -> text)\"] - df[\"I(text -> summary)\"]\n",
    "\n",
    "COL = 'I(summary -> text) - I(text -> summary)'\n",
    "for task in tasks_names:\n",
    "    for metric in ['proba_of_error', 'l2', 'l1', 'kl', 'dot', 'proba_of_error']:\n",
    "        print(task)\n",
    "\n",
    "        sns.set_theme(style=\"whitegrid\")\n",
    "        # g =sns.lmplot(data=df, x=\"I(summary -> text)\", y=f\"{task}/{metric}\", col=\"metadata/Dataset name\", facet_kws={'sharey': False, 'sharex': False}, scatter=False)\n",
    "        # g.map_dataframe(sns.scatterplot, x=\"I(summary -> text)\", y=f\"{task}/{metric}\", alpha=0.7, hue=\"metadata/Model name\", s=100)\n",
    "        g = sns.FacetGrid(df, col=\"metadata/Dataset name\", sharey=False, sharex=False, col_wrap=2, height=4, aspect=1.5)\n",
    "        g.map_dataframe(custom_reg_plot, x=COL, y=f\"{task}/{metric}\", hue=\"metadata/Decoding size\", style='metadata/Model name')\n",
    "\n",
    "\n",
    "\n",
    "        # change y title to be more readable\n",
    "        g.set(ylabel=metric)\n",
    "\n",
    "        # put legend outside, under the plot in the center\n",
    "        g.add_legend(loc='lower center', bbox_to_anchor=(0.20, -0.30), ncol=3)\n",
    "\n",
    "        # save figure\n",
    "        path = f\"../../../papers/Mutual-information-for-summarization/img/ablation_decoding_size/{task}_{metric}.png\"\n",
    "        # create parent\n",
    "        Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "        plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "        plt.clf()\n",
    "        plt.close('all')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1194a29fa4abc8a3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "SMALL_SIZE = 12\n",
    "MEDIUM_SIZE = 14\n",
    "BIGGER_SIZE = 18\n",
    "\n",
    "plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=SMALL_SIZE)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n",
    "plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n",
    "\n",
    "palette = sns.color_palette(\"viridis\", len(df['metadata/Decoding size'].unique()))\n",
    "\n",
    "\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette=palette)\n",
    "    return ax\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df[\"I(summary -> text) - I(text -> summary)\"] =  df[\"I(summary -> text)\"] - df[\"I(text -> summary)\"]\n",
    "\n",
    "# COL = 'I(summary -> text) - I(text -> summary)'\n",
    "COL = \"I(summary -> text)\"\n",
    "\n",
    "tasks_sh = set([c.split('/')[1] for c in df.columns if \"SH\" in c])\n",
    "metrics = set([c.split('/')[-1] for c in df.columns if \"SH\" in c])\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, len(tasks_sh) // 2, figsize=(15, 7), sharey=True, sharex=False, dpi=300)\n",
    "axes = axes.flatten()\n",
    "\n",
    "toplot = df.rename(columns={\"metadata/Decoding size\": \"Decoding size\", \"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "for tidx, task in enumerate(tasks_sh):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "    custom_reg_plot(data=toplot, x=COL, y=f\"SHMetric/{task}/proba_1\", hue=\"Decoding size\", style='Model name', ax=axes[tidx])\n",
    "    # annotate with r value\n",
    "    axes[tidx].annotate(f\"r={toplot[COL].corr(df[f'SHMetric/{task}/proba_1']):.2f}\", xy=(0.05, 0.95), xycoords='axes fraction', fontsize=16,\n",
    "                        horizontalalignment='left', verticalalignment='top')\n",
    "\n",
    "    # change y title to be more readable\n",
    "    axes[tidx].set_ylabel(\"P(yes)\", fontsize=16)\n",
    "    axes[tidx].set_xlabel(COL, fontsize=16)\n",
    "\n",
    "    # add title\n",
    "    axes[tidx].set_title(f\"{task}\", fontsize=20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# add global legend\n",
    "handles, labels = axes[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.25), ncol=3, fontsize=16)\n",
    "# remove all legends\n",
    "for ax in axes:\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "fig.tight_layout()\n",
    "# save figure\n",
    "path = f\"../../../papers/Mutual-information-for-summarization/img/ablation_decoding_size/cnn_dailymail_sh_metrics.png\"\n",
    "# create parent\n",
    "Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "# plt.clf()\n",
    "#plt.close('all')\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "11f01c9ae6907eb7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tasks_names"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9463f7a607c5ed5b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "tasks_names\n",
    "\n",
    "map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\" : \"Sentiment analysis\", \n",
    "             \"wesleyacheng_news-topic-classification-with-bert\" : \"Topic classification\",\n",
    "             \"roberta-base-openai-detector\" : \"GPT detector\",\n",
    "             }\n",
    "# select only the tasks we want\n",
    "\n",
    "# create a discrete sequential color palette with viridis\n",
    "palette = sns.color_palette(\"viridis\", len(df['metadata/Decoding size'].unique()))\n",
    "\n",
    "\n",
    "\n",
    "def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "    sns.regplot(data=data, x=x, y=y, ci=95, scatter=False, ax=ax, x_ci='sd')\n",
    "    sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=100, ax=ax, **kwargs, palette=palette)\n",
    "    return ax\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(len(map_tasks), 2, figsize=(10, 7), sharey=False, sharex=True, dpi=300)\n",
    "\n",
    "for tidx, task in enumerate(map_tasks.keys()):\n",
    "    topplot = df\n",
    "    # rename columns\n",
    "    topplot = topplot.rename(columns={\"metadata/Decoding size\": \"Decoding size\", \"metadata/Model name\": \"Model name\", \"metadata/Decoding config\": \"Decoding config\"})\n",
    "    \n",
    "    custom_reg_plot(data=topplot, x=\"I(summary -> text)\", y=f\"{task}/proba_of_error\", hue=\"Decoding size\", style='Model name', ax=axes[tidx, 0])\n",
    "    custom_reg_plot(data=topplot, x=\"I(summary -> text)\", y=f\"{task}/kl\", hue=\"Decoding size\", style='Model name', ax=axes[tidx, 1])\n",
    "    \n",
    "    # annotate with r value\n",
    "    axes[tidx, 0].annotate(f\"r={topplot['I(summary -> text)'].corr(df[f'{task}/proba_of_error']):.2f}\", xy=(0.05, 0.2), xycoords='axes fraction', fontsize=12,\n",
    "                        horizontalalignment='left', verticalalignment='top')\n",
    "    axes[tidx, 1].annotate(f\"r={topplot['I(summary -> text)'].corr(df[f'{task}/kl']):.2f}\", xy=(0.05, 0.1), xycoords='axes fraction', fontsize=12,)\n",
    "    \n",
    "\n",
    "    \n",
    "    # add title\n",
    "    axes[tidx, 0].set_title(map_tasks[task], fontsize=20)\n",
    "    axes[tidx, 1].set_title(map_tasks[task], fontsize=20)\n",
    "    \n",
    "    # add y label\n",
    "    axes[tidx, 0].set_ylabel(\"P(error)\", fontsize=16)\n",
    "    axes[tidx, 1].set_ylabel(\"KL\", fontsize=16)\n",
    "    \n",
    "\n",
    "\n",
    "# add global legend\n",
    "handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.3), ncol=2, fontsize=14)\n",
    "# remove all legends\n",
    "for ax in axes.flatten():\n",
    "    ax.get_legend().remove()\n",
    "\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "# save figure\n",
    "path = f\"../../../papers/Mutual-information-for-summarization/img/ablation_decoding_size/classification.png\"\n",
    "# create parent\n",
    "Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1e57a977acafcbbe"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f637faaa50a43457"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2e558ebefb2e11b0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "1267d43147931e73"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "dc0bc187b8f6c57"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ef97a974aefd1e",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "SUBSET_NAME = \"multilingual_mi2\"\n",
    "\n",
    "df_mis = pd.read_csv('../../output/multilingual/mis2.csv')\n",
    "df_rouge = pd.read_csv('../../output/multilingual/common.csv')\n",
    "df_classifiers = pd.read_csv('../../output/multilingual/classifiers.csv')\n",
    "model_sizes = pd.read_csv('../../output/model_sizes.csv')\n",
    "\n",
    "df_bert = pd.read_csv('../../output/multilingual/bert.csv')\n",
    "df_emb = pd.read_csv('../../output/multilingual/emb.csv')\n",
    "df_shm = pd.read_csv('../../output/multilingual/shm.csv')\n",
    "\n",
    "\n",
    "df_rouge['source'] = df_rouge[\"Unnamed: 0\"]\n",
    "df_classifiers['source'] = df_classifiers[\"Unnamed: 0\"]\n",
    "df_mis['source'] = df_mis[\"filename\"]\n",
    "df_bert['source'] = df_bert[\"Unnamed: 0\"]\n",
    "df_emb['source'] = df_emb[\"Unnamed: 0\"]\n",
    "df_shm['source'] = df_shm[\"Unnamed: 0\"]\n",
    "\n",
    "\n",
    "# merge all dataframes on source\n",
    "df_comprehensive = df_mis.merge(df_rouge, on='source', how=\"outer\")\n",
    "df_comprehensive = df_comprehensive.merge(df_classifiers, on='source')\n",
    "# merge emb and bert\n",
    "df_comprehensive = df_comprehensive.merge(df_bert, on='source')\n",
    "df_comprehensive = df_comprehensive.merge(df_emb, on='source')\n",
    "df_comprehensive = df_comprehensive.merge(df_shm, on='source')\n",
    "\n",
    "\n",
    "\n",
    "df_comprehensive['metadata/Model name'] = df_comprehensive['source'].apply(lambda x: x.split('-_-')[0])\n",
    "df_comprehensive['metadata/Decoding config'] = df_comprehensive['source'].apply(lambda x: x.split('-_-')[2])\n",
    "df_comprehensive['metadata/Dataset name'] = df_comprehensive['source'].apply(lambda x: x.split('-_-')[1])\n",
    "\n",
    "\n",
    "# remove all columns with gold\n",
    "df_comprehensive = df_comprehensive[[c for c in df_comprehensive.columns if \"gold\" not in c]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df_comprehensive = df_comprehensive.merge(model_sizes, on='metadata/Model name')\n",
    "\n",
    "# replace first _ in model by /\n",
    "df_comprehensive['metadata/Model name'] = df_comprehensive['metadata/Model name'].apply(\n",
    "    lambda x: x.replace('_', '/', 1))\n",
    "\n",
    "# drop columns with _x or _y\n",
    "df_comprehensive = df_comprehensive[[c for c in df_comprehensive.columns if not c.endswith(\"_x\") and not c.endswith(\"_y\")]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(df_comprehensive['metadata/Dataset name'].unique())\n",
    "\n",
    "datasets = ['xlsum_fra', 'xlsum_spa', 'mlsum_fra', 'mlsum_spa']\n",
    "df_comprehensive = df_comprehensive[df_comprehensive['metadata/Dataset name'].isin(datasets)]\n",
    "\n",
    "to_drop=[c for c in df_comprehensive.columns if \"large-openai\" in c]\n",
    "df_comprehensive = df_comprehensive.drop(to_drop, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_comprehensive"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7283675c67a0455"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4af84d7906749ec7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b9dcaaed32ad0c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bbfa38749d07828c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2cb013240b563db2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f611943c9aeb1c6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e755bab1b69b02bb",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d769f537ff4f2f94",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3a6b58e63420ba4f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e0062e1fd6958f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# sns.lmplot(data=df_comprehensive, x='metadata/#params', y='SHMetric/Main ideas/proba_1', col=\"metadata/Dataset name\",\n",
    "         #  hue=\"metadata/Model name\", height=5, sharey=False)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999a008a63ce4ec9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_comprehensive.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bb61b3451a84c2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "SHMS = [c for c in df_comprehensive.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "\n",
    "task_map = {'wesleyacheng_news-topic-classification-with-bert/proba_of_success' : \"Topic\", 'SamLowe_roberta-base-go_emotions/proba_of_success': \"Emotions\", 'mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis/proba_of_success' : \"Sentiment Analysis\", 'roberta-base-openai-detector/proba_of_success' : \"GPT Detector\", 'manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1/proba_of_success' : \"Policy\", 'embeddings/sentence-transformers_all-mpnet-base-v2/dot' : \"MPNET\", 'embeddings/sentence-transformers_all-MiniLM-L6-v2/dot' : \"all-MiniLM\", 'embeddings/sentence-transformers_paraphrase-MiniLM-L6-v2/dot' : \"Paraphrase\", 'common/rougeLsum' : \"ROUGE-L\", 'common/BERTScore' : \"BERTScore\"} | {c: c.split('/')[1] for c in SHMS}\n",
    "\n",
    "\n",
    "def make_correlation_table(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # df = df[df['metadata/Decoding config'].str.contains(\"100\") | df['metadata/Decoding config'].str.contains(\"50\")]\n",
    "    \n",
    "    \n",
    "\n",
    "    ROUGES = [\"common/rougeLsum\", \"common/BERTScore\"]\n",
    "    MI = ['I(summary -> text)', 'I(text -> summary)']\n",
    "    # SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "    # SHM_interesting = [c for c in SHM if \"Attribution\" in c or \"Main ideas\" in c]\n",
    "\n",
    "    SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "    SHM_interesting = [c for c in SHM if \"Attribution\" in c or \"Main ideas\" in c]\n",
    "   \n",
    "    \n",
    "    embeddings = [c for c in df.columns if \"embedding\" in c]\n",
    "\n",
    "    # map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "    #              \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "    #              # \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Policy classification\",\n",
    "    #              # \"jonaskoenig_topic_classification_04\" : \"Topic classification\",\n",
    "    #              \"SamLowe_roberta-base-go_emotions\" : \"Emotion classification\",\n",
    "    #              }\n",
    "    \n",
    "    map_tasks = {c: f\"Classif-{k}\" for k,c in enumerate(df.columns.unique()) if \"proba_of_error\" in c}\n",
    "    \n",
    "    print(map_tasks)\n",
    "\n",
    "    embedding_map = {c : f\"Emb-{k}\" for k,c in enumerate(embeddings)}\n",
    "\n",
    "    classification_tasks = [c for c in map_tasks.keys()]\n",
    "\n",
    "    # make proba_of_error proba_of_success\n",
    "    df[classification_tasks] = 1 - df[classification_tasks]\n",
    "    # rename\n",
    "    df = df.rename(columns={c: '/'.join(c.split('/')[:-1]) + \"/proba_of_success\" for c in map_tasks.keys()})\n",
    "\n",
    "    classification_tasks = ['/'.join(c.split('/')[:-1]) + \"/proba_of_success\" for c in map_tasks.keys()]\n",
    "    \n",
    "    embedding_tasks = [c for c in df.columns if \"embedding\" in c and \"dot\" in c]\n",
    "\n",
    "    datasets = set(df['metadata/Dataset name'].dropna().unique())\n",
    "\n",
    "    # create a dataframe with the correlation between MI and ROUGE and the SHmetrics, grouped by dataset\n",
    "    df_corr = pd.DataFrame(columns=['Dataset name', 'Metric', 'Correlation'])\n",
    "    \n",
    "    \n",
    "    for dataset in datasets:\n",
    "        # select dataset\n",
    "        df_dataset = df[df['metadata/Dataset name'] == dataset]\n",
    "        df_dataset = df_dataset[ROUGES + SHM + MI + classification_tasks + embedding_tasks].corr(\"kendall\")\n",
    "        # add dataset name\n",
    "        df_dataset['Dataset name'] = dataset\n",
    "\n",
    "        # add metric name\n",
    "        df_dataset['Metric'] = df_dataset.index\n",
    "\n",
    "        # melt dataframe\n",
    "        df_dataset = df_dataset.melt(id_vars=['Dataset name', 'Metric'], var_name=\"Correlation\", value_name=\"Value\")\n",
    "\n",
    "        # append to main dataframe\n",
    "        df_corr = df_corr.append(df_dataset)\n",
    "        \n",
    "\n",
    "    def rename_metrics(x):\n",
    "        splits = x.split('/')\n",
    "\n",
    "        if len(splits) == 1:\n",
    "            if splits[0] == \"I(summary -> text)\":\n",
    "                return \"$I(S;T)$\"\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            if x in task_map:\n",
    "                return task_map[x]\n",
    "            else:\n",
    "                return x\n",
    "\n",
    "    df_corr = df_corr.pivot(index=['Dataset name', 'Metric'], columns='Correlation', values='Value')\n",
    "    \n",
    "\n",
    "    # Keep shmetric only in columns\n",
    "    df_corr = pd.concat(\n",
    "        {'SH.': df_corr[[c for c in df_corr.columns if \"SHMetric\" in c]], 'CT.': df_corr[classification_tasks], 'Emb.' : df_corr[embedding_tasks], \"Common\" : df_corr[ROUGES]}, axis=1)\n",
    "\n",
    "    idx = pd.IndexSlice\n",
    "    # Select index to be displayed\n",
    "    df_corr = df_corr.loc[idx[:, ['I(summary -> text)'] + ROUGES + SHM_interesting], :]\n",
    "    # df_corr = df_corr.dropna()\n",
    "\n",
    "    df_corr = df_corr.reset_index()\n",
    "    # rename columns\n",
    "    #df_corr.columns = pd.MultiIndex.from_tuples(\n",
    "    #    [(c[0].replace('_', '-'), rename_metrics(c[0])) for c in df_corr.columns])\n",
    "\n",
    "    \n",
    "    # rename Metric\n",
    "    df_corr[('Metric', '')] = df_corr[('Metric', '')].apply(rename_metrics)\n",
    "\n",
    "    df_corr = df_corr.set_index([\"Dataset name\", 'Metric'])\n",
    "    df_corr = df_corr.sort_index()\n",
    "\n",
    "    # Remove \"_\" from column names\n",
    "\n",
    "    return df_corr\n",
    "\n",
    "\n",
    "table = make_correlation_table(df_comprehensive).transpose()\n",
    "\n",
    "# take average over the first level of columns\n",
    "\n",
    "table = table.groupby(level=1, axis=1).mean()\n",
    "\n",
    "table.index = pd.MultiIndex.from_tuples([(c[0], task_map[c[1]]) for c in table.index])\n",
    "\n",
    "\n",
    "display(table)\n",
    "\n",
    "\n",
    "# table.columns = pd.MultiIndex.from_tuples([(c[0].replace('_', '-'), c[1]) for c in table.columns])\n",
    "\n",
    "style = table.style\n",
    "\n",
    "style = style.format(precision=2)\n",
    "style = style.format_index(escape=\"latex\", axis=0)\n",
    "\n",
    "# highlight max for each dataset with bfseries\n",
    "# list_datasets = set(table.columns.get_level_values(0))\n",
    "#list_metrics = set(table.columns.get_level_values(1))\n",
    "idx = pd.IndexSlice\n",
    "# for dataset in list_datasets:\n",
    "#    style = style.highlight_max(axis=1, subset=(idx[:], idx[dataset, :]), props='bfseries:')\n",
    "\n",
    "# add background gradient\n",
    "style = style.background_gradient(cmap='viridis', vmin=0.2, vmax=1)\n",
    "\n",
    "# convert to latex\n",
    "path = f\"../../../papers/Mutual-information-for-summarization/tables/{SUBSET_NAME}_correlation_table_full.tex\"\n",
    "# create parent\n",
    "Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "latex_code = style.to_latex(clines=\"skip-last;data\", sparse_index=True, sparse_columns=True,\n",
    "                            caption=\"Correlation between MI and ROUGE, and Seahorse metrics and probability of success of the classifcation task, grouped by datasets for non-trivial decoding strategies. SH. stands for Seahorse metrics and CT. for classification tasks.\",\n",
    "                            label=\"tab:correlation_table\", environment=\"table\", hrules=True, convert_css=True, multicol_align=\"c\")\n",
    "\n",
    "import re\n",
    "\n",
    "# add a resize box around the tabular\n",
    "latex_code = re.sub(r\"\\\\begin{tabular}\", r\"\\\\resizebox{0.5\\\\textwidth}{!}{\\\\begin{tabular}\", latex_code)\n",
    "latex_code = re.sub(r\"\\\\end{tabular}\", r\"\\\\end{tabular}}\", latex_code)\n",
    "\n",
    "# add centering to the table environment\n",
    "latex_code = re.sub(r\"\\\\begin{table}\", r\"\\\\begin{table}\\\\centering\", latex_code)\n",
    "\n",
    "# save latex code\n",
    "with open(path, 'w') as f:\n",
    "    f.write(latex_code)\n",
    "    \n",
    "\n",
    "print(latex_code)\n",
    "\n",
    "print([c[1] for c in table.index])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "74ff9f68e0bda405"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "\n",
    "# radar chart function\n",
    "\n",
    "def make_radar_chart(df, indexes, columns, ax, ylim, yticks, legend):\n",
    "    df = df.copy()\n",
    "\n",
    "    # retrieve data\n",
    "    data = df.loc[indexes, columns]\n",
    "    \n",
    "    # data = data.sort_index(axis=1)\n",
    "    \n",
    "    # make a plot for each row\n",
    "\n",
    "\n",
    "    # to 0 if negative\n",
    "    # values = np.array([0.01 if v < 0 else v for v in values])\n",
    "    # get columns\n",
    "    b_angles = [n / float(len(columns)) * 2 * math.pi for n in range(len(columns))]\n",
    "    \n",
    "    angles = b_angles + b_angles[:1]\n",
    "    b_angles = b_angles + b_angles[:1]\n",
    "\n",
    "    # dic with angle for each column\n",
    "    angles_dict = {c: a for c, a in zip(columns, b_angles)}\n",
    "\n",
    "\n",
    "    color_list = ['tab:green', 'tab:orange', 'tab:blue', 'tab:red', 'tab:purple', 'tab:brown', 'tab:pink',\n",
    "              'tab:gray', 'tab:olive', 'tab:cyan']\n",
    "    \n",
    "    category_color_list = ['cornflowerblue', 'indianred', 'lightgreen', 'lightcoral', 'lightpink', 'lightgrey', 'lightyellow', 'lightcyan', 'lightseagreen']\n",
    "    category_hat_list = ['/', '\\\\', '|', '-', '+', 'x', 'o', 'O', '.']\n",
    "    \n",
    "    last_angle = 0\n",
    "    for k, level_0 in enumerate(data.columns.get_level_values(0).unique()):\n",
    "        print(level_0)\n",
    "        print(data[level_0].columns)\n",
    "        \n",
    "        start_angle =  angles_dict[(level_0, data[level_0].columns[0])] \n",
    "        end_angle = angles_dict[(level_0, data[level_0].columns[-1])]\n",
    "\n",
    "        if start_angle > end_angle:\n",
    "            start_angle, end_angle = end_angle, start_angle\n",
    "\n",
    "        start_angle = start_angle - 1/(float(len(columns))) * math.pi\n",
    "        \n",
    "        # if k == len(data.columns.get_level_values(0).unique()) - 1:\n",
    "        #     end_angle = 2 * math.pi - 1/(float(len(columns))) * math.pi\n",
    "        # else:\n",
    "        end_angle = end_angle + 1/(float(len(columns))) * math.pi\n",
    "            \n",
    "        langles = np.arange(start_angle, end_angle, 0.01)\n",
    "\n",
    "        ax.fill_between(langles,  0.9*np.ones(len(langles)), 1.2*np.ones(len(langles)), alpha=0.6, color=category_color_list[k], label=level_0, hatch=category_hat_list[k])\n",
    "\n",
    "    \n",
    "    for k, (idx, row) in enumerate(data.iterrows()):\n",
    "        # get values\n",
    "        values = row.values\n",
    "        values = np.concatenate((values, [values[0]]))\n",
    "        \n",
    "        # plot\n",
    "        if \"ROUGE\" in idx:\n",
    "            idx = \"ROUGE-L\"\n",
    "        elif \"BERT\" in idx:\n",
    "            idx = \"BERTScore\"\n",
    "        \n",
    "\n",
    "        # fill\n",
    "        if \"I(S;T)\" in idx:\n",
    "            ax.plot(angles, values, linewidth=5, linestyle='solid', label=idx, color=color_list[k])\n",
    "            ax.fill(angles, values, alpha=0.1, color=color_list[k])\n",
    "        else:\n",
    "            ax.plot(angles, values, linewidth=3, linestyle='--', label=idx, color=color_list[k])\n",
    "            ax.fill(angles, values, alpha=0.1, color=color_list[k])\n",
    "\n",
    "        # add legend\n",
    "\n",
    "        # add grid\n",
    "    ax.grid(True, which='both', axis='both', linestyle='solid')\n",
    "    \n",
    "\n",
    "    # set xticks\n",
    "    ax.set_xticks(angles[:-1])\n",
    "\n",
    "    # set xtick labels\n",
    "    cc = []\n",
    "    for c in columns:\n",
    "        cc.append(task_map[c[1]])\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "\n",
    "    ax.set_xticklabels(cc, fontsize=20, fontweight='bold')\n",
    "    \n",
    "\n",
    "    # set yticks\n",
    "    ax.set_yticks(yticks)\n",
    "\n",
    "    # set ytick labels\n",
    "    ax.set_yticklabels(yticks, fontsize=16)\n",
    "\n",
    "    # set ylim\n",
    "    ax.set_ylim(ylim)\n",
    "        \n",
    "        \n",
    "# draw a circle at y=0\n",
    "    ax.plot(np.linspace(0, 2 * math.pi, 100), np.zeros(100), linestyle='--', color='black', linewidth=2, label=\"Correlation = 0\")\n",
    "    \n",
    "    ax.set_axisbelow(False)\n",
    "\n",
    "table = make_correlation_table(df_comprehensive).transpose()\n",
    "\n",
    "datasets = ['xlsum_fra', 'xlsum_spa', 'mlsum_fra', 'mlsum_spa']\n",
    "dddf = table[datasets[0]].transpose()\n",
    "\n",
    "\n",
    "columns = [c for c in dddf.columns]\n",
    "print(columns)\n",
    "\n",
    "fig, ax = plt.subplots(1, len(datasets), figsize=(40, 20), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for k, d in enumerate(datasets):\n",
    "    dddf = table[d].transpose()\n",
    "    make_radar_chart(df=dddf, indexes=dddf.index, columns=columns, ax=ax[k], ylim=(-0.6, 1.1), yticks=[-0.6, -0.2, 0.2, 0.6, 1.0], legend=False)\n",
    "    \n",
    "    ax[k].set_title(d, fontsize=22, fontweight='bold')\n",
    "    \n",
    "# make global legend\n",
    "# fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, 0.2), ncol=3, fontsize=20)\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"../../../papers/Mutual-information-for-summarization/img/{SUBSET_NAME}_radar_chart_all.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "# columns = [c for c in dddf.columns if \"SH.\" in c]\n",
    "# \n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "# make_radar_chart(df=dddf, indexes=dddf.index, columns=columns, ax=ax, ylim=(-1, 1), yticks=[-1, -0.6, -0.2, 0.2, 0.6, 1.0])\n",
    "# \n",
    "# \n",
    "# columns = [c for c in dddf.columns if \"CT.\" in c]\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "# make_radar_chart(df=dddf, indexes=dddf.index, columns=columns, ax=ax, ylim=(0, 1), yticks=[0.2, 0.6, 0.8, 1.0]) "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eafb612f7fd506b7"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2e0c8f104897af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "ROUGES = [\"common/rougeLsum\"]\n",
    "MI = ['I(summary -> text)', 'I(text -> summary)', 'max mi', 'min mi']\n",
    "SHM = [c for c in df_comprehensive.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "# keep only Attribution, Main idea, Conciseness\n",
    "SHM = [c for c in SHM if \"Attribution\" in c or \"Main ideas\" in c or \"Conciseness\" in c]\n",
    "\n",
    "map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "             \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "             \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Topic classification\",\n",
    "             }\n",
    "\n",
    "classification_tasks_error = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "classification_tasks = [c + \"/proba_of_success\" for c in map_tasks.keys()]\n",
    "\n",
    "# make proba_of_error proba_of_success\n",
    "df_comprehensive[classification_tasks] = 1 - df_comprehensive[classification_tasks_error]\n",
    "\n",
    "\n",
    "def plot_multiple_datasets_correlations(df, COLS, metric, name):\n",
    "    sns.set_theme(style=\"whitegrid\")\n",
    "    df = df.copy()\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")]\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "\n",
    "    datasets = set(df['metadata/Dataset name'].dropna().unique())\n",
    "    datasets -= set(['peer_read', 'arxiv', 'rotten_tomatoes'])\n",
    "\n",
    "    fig, axes = plt.subplots(len(datasets), len(COLS), figsize=(10, 10), sharey=False, sharex=False, dpi=300)\n",
    "\n",
    "    def rename_cols(x):\n",
    "        if \"SHMetric\" in x:\n",
    "            return x.split('/')[1]\n",
    "        else:\n",
    "            return map_tasks[x.split('/')[0]]\n",
    "\n",
    "    for idx, col in enumerate(COLS):\n",
    "        for didx, ds in enumerate(datasets):\n",
    "            group = df[df['metadata/Dataset name'] == ds]\n",
    "            sns.set_theme(style=\"whitegrid\")\n",
    "\n",
    "            sns.regplot(data=group, x=metric, y=col, ax=axes[didx, idx], x_ci=None, ci=False, scatter=False,\n",
    "                        line_kws={'alpha': 0.5, 'linewidth': 5})\n",
    "            sns.scatterplot(data=group, x=metric, y=col, hue='metadata/Model name', style='metadata/Model name',\n",
    "                            ax=axes[didx, idx], palette='tab20', s=300)\n",
    "\n",
    "            axes[didx, idx].set_xlabel(\"\")\n",
    "            if didx == 0:\n",
    "                axes[didx, idx].set_title(rename_cols(col), fontsize=22, fontweight='bold')\n",
    "\n",
    "            axes[didx, idx].set_ylabel(\"\")\n",
    "            if idx == 0:\n",
    "                axes[didx, idx].set_ylabel(ds, fontsize=22, fontweight='bold')\n",
    "\n",
    "            # make xtick labels bigger\n",
    "            axes[didx, idx].tick_params(axis='x', labelsize=18)\n",
    "            axes[didx, idx].tick_params(axis='y', labelsize=18)\n",
    "\n",
    "            # add grid\n",
    "            axes[didx, idx].grid(True, which='both', axis='both', linestyle='--')\n",
    "\n",
    "    # global legend below the figure\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.22), ncol=2, fontsize=14)\n",
    "    # remove all legends\n",
    "    for ax in axes.flatten():\n",
    "        ax.get_legend().remove()\n",
    "\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/multiple_datasets_correlations_{name}.png\"\n",
    "    # create parent\n",
    "\n",
    "    fig.tight_layout()\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_multiple_datasets_correlations(df_comprehensive, COLS=SHM, metric=\"I(summary -> text)\", name=\"shmetrics_full_mi\")\n",
    "plot_multiple_datasets_correlations(df_comprehensive, COLS=classification_tasks, metric=\"I(summary -> text)\",\n",
    "                                   name=\"classification_tasks_full_mi\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# plot_multiple_datasets_correlations(df_comprehensive, COLS=SHM, metric=\"common/rougeLsum\", name=\"shmetrics_full_rouge\")\n",
    "# plot_multiple_datasets_correlations(df_comprehensive, COLS=classification_tasks,metric=\"common/rougeLsum\", name=\"classification_tasks_full_rouge\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12269da2762a8da5",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def plot_classification_tasks_proba_kl(df, dataset, legend=False, metric=\"I(summary -> text)\"):\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")].copy()\n",
    "    df = df[df['metadata/Dataset name'] == dataset]\n",
    "\n",
    "    df = df[~df['metadata/Decoding config'].isin([f\"beam_sampling_{k}\" for k in [5, 10, 20, 50]])]\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "                 \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "                 \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Topic classification\",\n",
    "                 }\n",
    "\n",
    "    # select only the tasks we want\n",
    "\n",
    "    # create a discrete sequential color palette with viridis\n",
    "\n",
    "    def custom_reg_plot(data, x=None, y=None, hue=None, ax=None, **kwargs):\n",
    "        sns.regplot(data=data, x=x, y=y, ci=None, scatter=False, ax=ax, x_ci='sd', line_kws={'linewidth': 5, 'alpha' : 0.5})\n",
    "        sns.scatterplot(data=data, x=x, y=y, hue=hue, alpha=1, s=500, ax=ax, **kwargs, palette=\"tab10\")\n",
    "        return ax\n",
    "\n",
    "    fig, axes = plt.subplots(len(map_tasks), 2, figsize=(10, 12), sharey=False, sharex=True, dpi=300)\n",
    "\n",
    "    for tidx, task in enumerate(map_tasks.keys()):\n",
    "        topplot = df\n",
    "        # rename columns\n",
    "        topplot = topplot.rename(\n",
    "            columns={\"metadata/Decoding size\": \"Decoding size\", \"metadata/Model name\": \"Model name\",\n",
    "                     \"metadata/Decoding config\": \"Decoding config\"})\n",
    "\n",
    "        custom_reg_plot(data=topplot, x=metric, y=f\"{task}/proba_of_error\", hue=\"Model name\",\n",
    "                        style='Model name', ax=axes[tidx, 0])\n",
    "        custom_reg_plot(data=topplot, x=metric, y=f\"{task}/kl\", hue=\"Model name\", style='Model name',\n",
    "                        ax=axes[tidx, 1])\n",
    "\n",
    "        # annotate with r value\n",
    "        axes[tidx, 0].annotate(f\"r={topplot[metric].corr(df[f'{task}/proba_of_error']):.2f}\",\n",
    "                               xy=(0.05, 0.2), xycoords='axes fraction', fontsize=12,\n",
    "                               horizontalalignment='left', verticalalignment='top')\n",
    "        axes[tidx, 1].annotate(f\"r={topplot[metric].corr(df[f'{task}/kl']):.2f}\", xy=(0.05, 0.1),\n",
    "                               xycoords='axes fraction', fontsize=12, )\n",
    "\n",
    "        # add title\n",
    "        axes[tidx, 0].set_title(map_tasks[task], fontsize=20, fontweight='bold')\n",
    "        axes[tidx, 1].set_title(map_tasks[task], fontsize=20, fontweight='bold')\n",
    "\n",
    "        # add y label\n",
    "        axes[tidx, 0].set_ylabel(\"P(error)\", fontsize=16, fontweight='bold')\n",
    "        axes[tidx, 1].set_ylabel(\"KL\", fontsize=16, fontweight='bold')\n",
    "\n",
    "        # remove x label:\n",
    "        axes[tidx, 0].set_xlabel(\"\")\n",
    "        axes[tidx, 1].set_xlabel(\"\")\n",
    "        \n",
    "        # make tick labels bigger\n",
    "        axes[tidx, 0].tick_params(axis='x', labelsize=16)\n",
    "        axes[tidx, 1].tick_params(axis='x', labelsize=16)\n",
    "        \n",
    "        axes[tidx, 0].tick_params(axis='y', labelsize=16)\n",
    "        axes[tidx, 1].tick_params(axis='y', labelsize=16)\n",
    "         \n",
    "        axes[tidx, 0].grid(True, which='both', axis='both', linestyle='--')\n",
    "        axes[tidx, 1].grid(True, which='both', axis='both', linestyle='--')\n",
    "\n",
    "    # add global legend\n",
    "    handles, labels = axes[0, 0].get_legend_handles_labels()\n",
    "    # fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.35), ncol=2, fontsize=16)\n",
    "    # remove all legends\n",
    "    for ax in axes.flatten():\n",
    "        ax.get_legend().remove()\n",
    "        \n",
    "    if legend:\n",
    "        fig.legend(handles, labels, loc='lower center', bbox_to_anchor=(0.5, -0.20), ncol=2, fontsize=16)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/classification_tasks/{dataset}_{metric}_classification_tasks_full.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_classification_tasks_proba_kl(df_comprehensive, dataset=\"cnn_dailymail\")\n",
    "plot_classification_tasks_proba_kl(df_comprehensive, dataset=\"rotten_tomatoes\")\n",
    "\n",
    "plot_classification_tasks_proba_kl(df_comprehensive, dataset=\"xsum\")\n",
    "plot_classification_tasks_proba_kl(df_comprehensive, dataset=\"multi_news\")\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb6a7af0fea65afc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "def plot_classification_tasks_proba(df, legend=False):\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")].copy()\n",
    "    df = df[~df['metadata/Dataset name'].str.contains('rotten')]\n",
    "\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "                 \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "                 \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Topic classification\",\n",
    "                 }\n",
    "    \n",
    "    tasks = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "    \n",
    "    ddf =pd.melt(df, id_vars=['metadata/Model name', 'metadata/Dataset name', 'metadata/#params', 'I(summary -> text)'], value_vars=tasks, var_name='Task', value_name='P(error)')\n",
    "    ddf = ddf.rename(columns={'metadata/Model name': 'Model', 'metadata/Dataset name': 'Dataset', 'metadata/#params': '#params'})\n",
    "    ddf['Task'] = ddf['Task'].map(lambda x: x.split('/')[0])\n",
    "    ddf['Task'] = ddf['Task'].map(lambda x: map_tasks[x])\n",
    "    \n",
    "    # legend below\n",
    "    g = sns.relplot(data=ddf, x=\"I(summary -> text)\", y=\"P(error)\", hue=\"Model\", style='Model', col=\"Task\", row=\"Dataset\", height=3, aspect=0.7, palette='tab10', facet_kws={'sharey': False, 'sharex': False, 'margin_titles':True}, s=500)\n",
    "    \n",
    "    # add regression line\n",
    "    g.map(sns.regplot, \"I(summary -> text)\", \"P(error)\", scatter=False, ci=0.95, line_kws={'linewidth': 10, 'alpha': 0.5})\n",
    "    \n",
    "    # make title bigger\n",
    "    #g.fig.suptitle(\"Probability of error vs $I(T,S)$\", fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # Column title format \n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\", size=20, fontweight='bold')\n",
    "    \n",
    "    # make ylabels bigger\n",
    "    g.set_ylabels(\"P(error)\", fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # make xlabels bigger\n",
    "    g.set_xlabels(\"$I(T,S)$\", fontsize=20, fontweight='bold')\n",
    "    \n",
    "    sns.move_legend(g, \"lower center\", bbox_to_anchor=(0.6, -0.3), ncol=2, fontsize=16)\n",
    "    \n",
    "\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/classification_tasks/classification_tasks_full_proba.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    g.fig.tight_layout()\n",
    "\n",
    "    g.fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_classification_tasks_proba(df_comprehensive)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4555be221c6366a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def plot_reg(x,y, **kwargs):\n",
    "    ax = plt.gca()\n",
    "    sns.regplot(x=x,y=y, ci=95, color=kwargs.get('color') , scatter=False, ax=ax, x_ci='sd', line_kws={'linewidth': 2, 'alpha' : 0.5}, truncate=True, label=kwargs.get('label'), robust=True)\n",
    "    return ax\n",
    "\n",
    "def plot_classification_tasks_proba_scatter(df, legend=False, metric=\"I(summary -> text)\"):\n",
    "    df = df[~df['metadata/Decoding config'].str.contains(\"short\")].copy()\n",
    "    df = df[df['metadata/Decoding config'].str.contains(\"200\")].copy()\n",
    "    \n",
    "    df = df[~df['metadata/Dataset name'].str.contains('rotten')]\n",
    "\n",
    "\n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "                 \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "                 \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Topic classification\",\n",
    "                 }\n",
    "\n",
    "    tasks = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "\n",
    "    ddf =pd.melt(df, id_vars=['metadata/Model name', 'metadata/Dataset name', 'metadata/#params', metric], value_vars=tasks, var_name='Task', value_name='P(error)')\n",
    "    ddf = ddf.rename(columns={'metadata/Model name': 'Model', 'metadata/Dataset name': 'Dataset', 'metadata/#params': '#params'})\n",
    "    ddf['Task'] = ddf['Task'].map(lambda x: x.split('/')[0])\n",
    "    ddf['Task'] = ddf['Task'].map(lambda x: map_tasks[x])\n",
    "\n",
    "    # legend below\n",
    "    # g = sns.relplot(data=ddf, x=\"I(summary -> text)\", y=\"P(error)\", style=\"Model\", hue='Dataset', col=\"Task\", height=5, aspect=0.7, palette='tab10', facet_kws={'sharey': False, 'sharex': False, #'margin_titles':True}, s=200, hue_order=['xsum', 'cnn_dailymail', 'multi_news'])\n",
    "    \n",
    "    # make facetgrid \n",
    "    g = sns.FacetGrid(data=ddf, col=\"Task\", hue='Dataset', height=7, aspect=1.2, palette='tab10', col_order=['Sentiment analysis', 'GPT detector', 'Topic classification'], sharex=False, sharey=False)\n",
    "    \n",
    "    # add scatter plot\n",
    "    g.map_dataframe(sns.scatterplot, metric, \"P(error)\", s=300, alpha=1, style='Model', hue_order=['xsum', 'cnn_dailymail', 'multi_news'])\n",
    "    \n",
    "    # add regression line\n",
    "    g.map(plot_reg, metric, \"P(error)\")\n",
    "\n",
    "    # make title bigger\n",
    "    #g.fig.suptitle(\"Probability of error vs $I(T,S)$\", fontsize=20, fontweight='bold')\n",
    "\n",
    "    # Column title format \n",
    "    g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name}\", size=20, fontweight='bold')\n",
    "\n",
    "    # make ylabels bigger\n",
    "    g.set_ylabels(\"P(error)\", fontsize=20, fontweight='bold')\n",
    "\n",
    "    # make xlabels bigger\n",
    "    g.set_xlabels(\"$I(T,S)$\", fontsize=20, fontweight='bold')\n",
    "    \n",
    "    # make xticks bigger\n",
    "    g.set_xticklabels(fontsize=16)\n",
    "    \n",
    "    # make yticks bigger\n",
    "    g.set_yticklabels(fontsize=16)\n",
    "    \n",
    "    handles, labels = g.axes[0, 0].get_legend_handles_labels()\n",
    "    \n",
    "    by_label = dict(zip(labels, handles))\n",
    "    # order by length of label\n",
    "    by_label = {k: v for k, v in sorted(by_label.items(), key=lambda item: len(item[0]))}\n",
    "    \n",
    "    # add legend\n",
    "    g.fig.legend(by_label.values(), by_label.keys(), loc='lower center', bbox_to_anchor=(0.5, -0.22), ncol=5, fontsize=20)\n",
    "\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/classification_tasks/classification_tasks_full_proba_scatter_{metric}.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    g.fig.tight_layout()\n",
    "\n",
    "    g.fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "plot_classification_tasks_proba_scatter(df_no_arxiv)\n",
    "plot_classification_tasks_proba_scatter(df_no_arxiv, metric=\"common/BERTScore\")\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a9031c851112bb53"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6cbe280377d67c84"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_model_comparison(df):\n",
    "    df = df.copy()\n",
    "    # remove dataset rotten tomatoes\n",
    "    df = df[df['metadata/Dataset name'] != 'rotten_tomatoes']\n",
    "    \n",
    "    # remove first part of model name\n",
    "    df['metadata/Model name'] = df['metadata/Model name'].apply(lambda x: \" \".join(x.split('/')[1:]))\n",
    "    \n",
    "    # sort by model size\n",
    "    df = df.sort_values(by=['metadata/IND/OOD', 'metadata/#params'])\n",
    "    ax = sns.barplot(data=df, x='metadata/Model name', y='I(summary -> text)', hue='metadata/Dataset name', orient='v', palette='tab10')\n",
    "    \n",
    "    # twin axes x with model size\n",
    "    ax2 = plt.twinx()\n",
    "    # log scale\n",
    "    ax2.set_yscale('log')\n",
    "    \n",
    "    sns.barplot(data=df, x='metadata/Model name', y='metadata/#params', orient='v',  ax=ax2, alpha=0.6, color='grey')\n",
    "    \n",
    "    # make ylim 40, 60\n",
    "    ax.set_ylim(40, 60)\n",
    "    \n",
    "    # rotate x labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=80)\n",
    "    \n",
    "    # rename labels\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"$I(T;S)$\", fontsize=20, )\n",
    "    ax2.set_ylabel(\"Size\", fontsize=20)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # add vertical lines for the best model on each dataset\n",
    "    for didx, dataset in enumerate(reversed(['xsum', 'cnn_dailymail', 'multi_news'])):\n",
    "        best_model = df[df['metadata/Dataset name'] == dataset].sort_values(by='I(summary -> text)').iloc[-1]\n",
    "        ax.axhline(y=best_model['I(summary -> text)'], color=f'C{didx}', linestyle='--', alpha=1, linewidth=3)\n",
    "        \n",
    "        \n",
    "    # put legend outside top \n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), ncol=3, fontsize=16)\n",
    "\n",
    "    \n",
    "\n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/model_comparison.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig = ax.get_figure()\n",
    "    # fig.tight_layout()\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "plot_model_comparison(df_comprehensive)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f68bfa88c341b7b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_correlation_matrix(df):\n",
    "    df = df.copy()\n",
    "    # remove dataset rotten tomatoes\n",
    "    df = df[df['metadata/Dataset name'] != 'rotten_tomatoes']\n",
    "    \n",
    "    # remove first part of model name\n",
    "    df['metadata/Model name'] = df['metadata/Model name'].apply(lambda x: \" \".join(x.split('/')[1:]))\n",
    "    \n",
    "    # sort by model size\n",
    "    df = df.sort_values(by=['metadata/IND/OOD', 'metadata/#params'])\n",
    "    \n",
    "    # select only the columns we want\n",
    "    ROUGES = [\"common/rougeLsum\"]\n",
    "    MI = ['I(summary -> text)']\n",
    "    SHM = [c for c in df.columns if \"SHMetric\" in c and \"proba_1\" in c]\n",
    "    # keep only Attribution, Main idea, Conciseness\n",
    "    SHM = [c for c in SHM if \"Attribution\" in c or \"Main ideas\" in c or \"Conciseness\" in c]\n",
    "    \n",
    "    map_tasks = {\"mrm8488_distilroberta-finetuned-financial-news-sentiment-analysis\": \"Sentiment analysis\",\n",
    "                 \"roberta-base-openai-detector\": \"GPT detector\",\n",
    "                 \"manifesto-project_manifestoberta-xlm-roberta-56policy-topics-context-2023-1-1\" : \"Topic classification\",\n",
    "                 }\n",
    "    \n",
    "    classification_tasks_error = [c + \"/proba_of_error\" for c in map_tasks.keys()]\n",
    "    classification_tasks = [c + \"/proba_of_success\" for c in map_tasks.keys()]\n",
    "    \n",
    "    # make proba_of_error proba_of_success\n",
    "    df[classification_tasks] = 1 - df[classification_tasks_error]\n",
    "\n",
    "    def rename_metrics(x):\n",
    "        splits = x.split('/')\n",
    "\n",
    "        if len(splits) == 1:\n",
    "            if splits[0] == \"I(summary -> text)\":\n",
    "                return \"$I(T,S)$\"\n",
    "            else:\n",
    "                return x\n",
    "        else:\n",
    "            if splits[0] in map_tasks.keys():\n",
    "                return map_tasks[splits[0]]\n",
    "            else:\n",
    "                if splits[1] == \"rougeLsum\":\n",
    "                    return \"ROUGE-L\"\n",
    "                else:\n",
    "                    return splits[1]\n",
    "\n",
    "    # select only the tasks we want\n",
    "    df = df[ROUGES + MI + SHM + classification_tasks]\n",
    "    \n",
    "    \n",
    "    # rename columns\n",
    "    df.columns = [rename_metrics(c) for c in df.columns]\n",
    "    \n",
    "    \n",
    "    corrs = df.corr(method='spearman')\n",
    "    \n",
    "    sns.set_theme(style=\"white\")\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    \n",
    "    mask = np.triu(np.ones_like(corrs, dtype=bool), k=1)\n",
    "    sns.heatmap(corrs, annot=True, cmap='Blues', robust=True, annot_kws={\"fontsize\": 16}, ax=ax, mask=mask, fmt='.1f', square=True)\n",
    "    \n",
    "    # resize yticks\n",
    "    ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16)\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=16)\n",
    "    \n",
    "    # save figure\n",
    "    path = f\"../../../papers/Mutual-information-for-summarization/img/correlation_matrix.png\"\n",
    "    # create parent\n",
    "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "plot_correlation_matrix(df_comprehensive)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "503dc0c04248f81b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "51a5f63e02f31487"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "98e8859403819754"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "6aa753e2625943c4"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "63c46c7b4b5306aa"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "5cc06ff7e784ab90"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
